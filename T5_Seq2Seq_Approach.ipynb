{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "685 Seq2Seq Approach.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://shivanandroy.com/fine-tune-t5-transformer-with-pytorch/"
      ],
      "metadata": {
        "id": "VgIFD_oiqijB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "izBi62wd5MDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f860abee-e8de-4ba7-d3ad-28ffdcf8c42d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 21.9 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 60.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 71.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 602 kB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.13.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KzrYgCmMcQl",
        "outputId": "2e41ea30-36d2-4f73-cca7-579fecf030ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning T5 for Text Normalization"
      ],
      "metadata": {
        "id": "5BHCeQK755t1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import difflib\n",
        "nltk.download('punkt')\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "path = 'drive/MyDrive/CS685'\n",
        "\n",
        "class GetDataset(Dataset):\n",
        "  def get_masked_text(self, idx, text, len):\n",
        "    text = str(text[idx])\n",
        "    # converting tab to space\n",
        "    text = \" \".join(text.split())\n",
        "    encoded_text = self.tokenizer.batch_encode_plus(\n",
        "        [text],\n",
        "        max_length=len,\n",
        "        pad_to_max_length=True,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\")\n",
        "    ids = encoded_text['input_ids'].squeeze()\n",
        "    mask = encoded_text[\"attention_mask\"].squeeze()\n",
        "    return ids.to(dtype=torch.long), mask.to(dtype=torch.long)\n",
        "  \n",
        "  def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataframe\n",
        "    self.source_len = source_len    \n",
        "    self.source_text = self.data[source_text]\n",
        "    self.target_len = target_len\n",
        "    self.target_text = self.data[target_text]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.target_text)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    source_ids, source_mask = self.get_masked_text(idx, self.source_text, self.source_len)\n",
        "    target_ids, target_mask = self.get_masked_text(idx, self.target_text, self.target_len)\n",
        "    return {\"source_ids\": source_ids.to(device), \"source_mask\": source_mask.to(device),\n",
        "            \"target_ids\": target_ids.to(device), \"target_mask\": target_mask.to(device)}"
      ],
      "metadata": {
        "id": "HV_KkGSokOzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fffb0ced-3d31-42be-c99b-d1a27623c146"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_params = {\n",
        "    \"MODEL\": \"t5-base\",\n",
        "    \"TRAIN_BATCH_SIZE\": 8,\n",
        "    \"VALID_BATCH_SIZE\": 8,\n",
        "    \"TRAIN_EPOCHS\": 3,\n",
        "    \"VAL_EPOCHS\": 1,\n",
        "    \"TEST_EPOCHS\": 1,\n",
        "    \"LEARNING_RATE\": 5e-5, #2e-5, 5e-6\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": 512,\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": 512,\n",
        "    \"SEED\": 42,\n",
        "}\n",
        "test_params = {\n",
        "    \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "    \"shuffle\": False,\n",
        "    \"num_workers\": 0,\n",
        "}\n"
      ],
      "metadata": {
        "id": "7WniQOyGt2P7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dissimilar_spans(orig_words, gt_words, pred_words):\n",
        "  gt_matcher = difflib.SequenceMatcher(a=orig_words, b=gt_words)\n",
        "  pred_matcher = difflib.SequenceMatcher(a=gt_words, b=pred_words)\n",
        "  orig_spans = []\n",
        "  gt_spans = []\n",
        "  pred_spans = []\n",
        "  mismatch_spans = []\n",
        "  for codes in gt_matcher.get_opcodes():\n",
        "    op,a_start,a_end,b_start,b_end = codes\n",
        "    if op == 'replace':\n",
        "      orig_spans.append(\" \".join(orig_words[a_start:a_end]))\n",
        "      gt_spans.append(\" \".join(gt_words[b_start:b_end]))\n",
        "\n",
        "  for codes in pred_matcher.get_opcodes():\n",
        "    op,a_start,a_end,b_start,b_end = codes\n",
        "    if op == 'replace':\n",
        "      pred_spans.append(\" \".join(pred_words[b_start:b_end]))\n",
        "      mismatch_spans.append(\" \".join(gt_words[a_start:a_end]))\n",
        "  \n",
        "  return orig_spans, gt_spans, pred_spans, mismatch_spans\n",
        "\n",
        "def get_stats_for_predictions(orig_text, gt_text, pred_text):\n",
        "  orig_words = nltk.word_tokenize(orig_text)\n",
        "  gt_words = nltk.word_tokenize(gt_text)\n",
        "  pred_words = nltk.word_tokenize(pred_text)\n",
        "  orig_words = [word.lower().strip() for word in orig_words]\n",
        "  gt_words = [word.lower().strip() for word in gt_words]\n",
        "  pred_words = [word.lower().strip() for word in pred_words]\n",
        "  correct_preds = []\n",
        "  wrong_preds = []\n",
        "  changed_orig_words = []\n",
        "  changed_gt_words = []\n",
        "  replaced_word_cnt = 0\n",
        "  correct_pred_cnt = 0\n",
        "  if len(orig_words)!= len(gt_words):\n",
        "    print(orig_text)\n",
        "    print(gt_text)\n",
        "  elif len(gt_words)!=len(pred_words):\n",
        "    orig_spans, gt_spans, pred_spans, mismatch_spans = get_dissimilar_spans(orig_words, gt_words, pred_words)\n",
        "    wrong_preds = pred_spans\n",
        "    changed_orig_words = orig_spans\n",
        "    changed_gt_words = gt_spans\n",
        "    replaced_word_cnt = len(gt_spans)\n",
        "    correct_pred_cnt = len(gt_spans) - len(mismatch_spans)\n",
        "    correct_preds = list(set(gt_spans)-set(mismatch_spans))\n",
        "  else:\n",
        "    for i in range(len(orig_words)):\n",
        "      orig_word = orig_words[i]\n",
        "      gt_word = gt_words[i]\n",
        "      pred_word = pred_words[i]\n",
        "      if orig_word != gt_word:\n",
        "        changed_orig_words.append(orig_word)\n",
        "        changed_gt_words.append(gt_word)\n",
        "        replaced_word_cnt = replaced_word_cnt+1\n",
        "        if pred_word == gt_word:\n",
        "          correct_preds.append(pred_word)\n",
        "          correct_pred_cnt = correct_pred_cnt+1\n",
        "        else:\n",
        "          wrong_preds.append(pred_word)\n",
        "\n",
        "  return {\"replaced_gt_words\":changed_gt_words,\n",
        "          \"replaced_original_words\": changed_orig_words,\n",
        "          \"replaced_word_count\": replaced_word_cnt,\n",
        "          \"correct_predictions\": correct_preds,\n",
        "          \"correct_prediction_count\": correct_pred_cnt,\n",
        "          \"wrong_predictions\": wrong_preds}"
      ],
      "metadata": {
        "id": "DKPyO7Pr-T40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy_df(input_df, pred_df):\n",
        "  df = pd.concat([input_df, pred_df], axis=1).drop(columns=['gt_text'])\n",
        "  df['text'] = df['text'].apply(lambda x : x.replace('denoise_text: ',''))\n",
        "  df[\"Stats\"] = df.apply(lambda x: get_stats_for_predictions(x[\"text\"], x[\"GT_Text\"],x[\"Predicted_Text\"]), axis = 1)\n",
        "  df = pd.concat([df.drop(['Stats'], axis=1), df['Stats'].apply(pd.Series)], axis=1)\n",
        "  return df"
      ],
      "metadata": {
        "id": "D-_6sCHQbPpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, tokenizer, model, loader, optimizer):\n",
        "  model.train()\n",
        "  loss_history = []\n",
        "  for i, data in enumerate(loader, 0):\n",
        "    y = data[\"target_ids\"]\n",
        "    y_ids = y[:, :-1].contiguous()\n",
        "    labels = y[:, 1:].clone().detach()\n",
        "    labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "    ids = data[\"source_ids\"]\n",
        "    mask = data[\"source_mask\"]\n",
        "    outputs = model(\n",
        "        input_ids=ids,\n",
        "        attention_mask=mask,\n",
        "        decoder_input_ids=y_ids,\n",
        "        labels=labels\n",
        "        )\n",
        "    loss = outputs[0]\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if i % 50 == 0:\n",
        "      print(\"Epoch: {}, Step: {}, Loss: {}\".format(epoch,i,loss))\n",
        "      loss_history.append(loss)\n",
        "  # plt.plot(range(len(loss_history)), loss_history)\n"
      ],
      "metadata": {
        "id": "SnEOCzUCkWQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_from_model(epoch, tokenizer, model, loader):\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(loader, 0):\n",
        "      y = data['target_ids']\n",
        "      ids = data['source_ids']\n",
        "      mask = data['source_mask']\n",
        "\n",
        "      # predicting correct sentences from noisy input\n",
        "      generated_ids = model.generate(\n",
        "          input_ids = ids,\n",
        "          attention_mask = mask,\n",
        "          max_length=512,\n",
        "          num_beams=2, #3,5\n",
        "          repetition_penalty=2.5,\n",
        "          length_penalty=1.0,\n",
        "          early_stopping=True\n",
        "          )\n",
        "      preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "      target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in y]\n",
        "      preds = [x.lstrip(\". \").lstrip(': ') for x in preds]\n",
        "      if i%150==0:\n",
        "        print('Completed {} Steps'.format(i))\n",
        "      \n",
        "      predictions.extend(preds)\n",
        "      actuals.extend(target)\n",
        "  return predictions, actuals"
      ],
      "metadata": {
        "id": "dG9ISLsMkXEl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def T5Trainer(\n",
        "    dataframe, source_text, target_text, model_params, output_dir\n",
        "):\n",
        "\n",
        "    torch.manual_seed(model_params[\"SEED\"])\n",
        "    print(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"]).to(device)\n",
        "\n",
        "    dataframe = dataframe[[source_text, target_text]]\n",
        "    display(dataframe.head(2))\n",
        "\n",
        "    train_size = 0.8\n",
        "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
        "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    train_dataset.to_csv(os.path.join(output_dir, \"train_set.csv\"), index=False, sep='\\t')\n",
        "    val_dataset.to_csv(os.path.join(output_dir, \"val_set.csv\"), index=False, sep='\\t')\n",
        "\n",
        "    print(f\"FULL Dataset: {dataframe.shape}\")\n",
        "    print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "    print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
        "\n",
        "    train_params = {\n",
        "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "        \"shuffle\": True,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    val_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "    training_set = GetDataset(\n",
        "        train_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "    val_set = GetDataset(\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
        "    )\n",
        "\n",
        "    print(f\"Starting Fine Tuning...\\n\")\n",
        "\n",
        "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "        train(epoch, tokenizer, model, training_loader, optimizer)\n",
        "\n",
        "    print(f\"Saving Model...\\n\")\n",
        "    # Saving the model after training\n",
        "    path = os.path.join(output_dir, \"model_files\")\n",
        "    model.save_pretrained(path)\n",
        "    tokenizer.save_pretrained(path)\n",
        "\n",
        "    # evaluating validation dataset\n",
        "    print(f\"Starting Validation...\\n\")\n",
        "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "        predictions, actuals = predict_from_model(epoch, tokenizer, model, val_loader)\n",
        "        final_df = pd.DataFrame({\"Predicted_Text\": predictions, \"GT_Text\": actuals})\n",
        "        accuracy_df = get_accuracy_df(val_dataset, final_df)\n",
        "        print(f\"Validation accuracy: {accuracy_df['correct_prediction_count'].sum()/accuracy_df['replaced_word_count'].sum()}\")\n",
        "        final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"), index=False, sep='\\t')\n",
        "    \n",
        "    print(f\"Validation Completed...\\n\")\n",
        "    print(f\"\"\"Model saved at {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n",
        "    print(f\"\"\"Generation on Validation data saved at {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")"
      ],
      "metadata": {
        "id": "ufFyDT3pkkYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(os.path.join(path, \"xsum_train_val.csv\"),sep='\\t')\n",
        "df['text'] = 'denoise_text: '+df['text']\n"
      ],
      "metadata": {
        "id": "HCV1U7-pkpe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "output_dir = os.path.join(path,\"t5_trained_model_\"+str(time.time()))\n",
        "os.mkdir(output_dir)\n",
        "T5Trainer(\n",
        "    dataframe=df,\n",
        "    source_text=\"text\",\n",
        "    target_text=\"gt_text\",\n",
        "    model_params=model_params,\n",
        "    output_dir=output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p0cQf3orlAtc",
        "outputId": "f2a370f2-15bc-44a7-9972-b0f0d7a188cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Model]: Loading t5-base...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>gt_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>denoise_text: A haul of wht is belved to be cocan wid a street value of £120,000 has been uncovered by engineerS dsantlng a scraPped jumooo jet.</td>\n",
              "      <td>A haul of what is believed to be cocaine with a street value of £120,000 has been uncovered by engineers dismantling a scrapped jumbo jet.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>denoise_text: Sevennnnn council-run car hms earmarked for closure t save £1.9m have been takEn over by a private company.</td>\n",
              "      <td>Seven council-run care homes earmarked for closure to save £1.9m have been taken over by a private company.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                               text  \\\n",
              "0  denoise_text: A haul of wht is belved to be cocan wid a street value of £120,000 has been uncovered by engineerS dsantlng a scraPped jumooo jet.   \n",
              "1  denoise_text: Sevennnnn council-run car hms earmarked for closure t save £1.9m have been takEn over by a private company.                          \n",
              "\n",
              "                                                                                                                                      gt_text  \n",
              "0  A haul of what is believed to be cocaine with a street value of £120,000 has been uncovered by engineers dismantling a scrapped jumbo jet.  \n",
              "1  Seven council-run care homes earmarked for closure to save £1.9m have been taken over by a private company.                                 "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL Dataset: (60000, 2)\n",
            "TRAIN Dataset: (48000, 2)\n",
            "TEST Dataset: (12000, 2)\n",
            "\n",
            "Starting Fine Tuning...\n",
            "\n",
            "Epoch: 0, Step: 0, Loss: 8.920101165771484\n",
            "Epoch: 0, Step: 50, Loss: 3.739185333251953\n",
            "Epoch: 0, Step: 100, Loss: 2.454301118850708\n",
            "Epoch: 0, Step: 150, Loss: 2.605936288833618\n",
            "Epoch: 0, Step: 200, Loss: 1.9391080141067505\n",
            "Epoch: 0, Step: 250, Loss: 2.093918561935425\n",
            "Epoch: 0, Step: 300, Loss: 1.5826873779296875\n",
            "Epoch: 0, Step: 350, Loss: 1.878882646560669\n",
            "Epoch: 0, Step: 400, Loss: 1.5180747509002686\n",
            "Epoch: 0, Step: 450, Loss: 1.6507668495178223\n",
            "Epoch: 0, Step: 500, Loss: 1.0915071964263916\n",
            "Epoch: 0, Step: 550, Loss: 1.3303786516189575\n",
            "Epoch: 0, Step: 600, Loss: 1.51410710811615\n",
            "Epoch: 0, Step: 650, Loss: 1.119321346282959\n",
            "Epoch: 0, Step: 700, Loss: 1.1974446773529053\n",
            "Epoch: 0, Step: 750, Loss: 1.1575895547866821\n",
            "Epoch: 0, Step: 800, Loss: 0.8258547782897949\n",
            "Epoch: 0, Step: 850, Loss: 1.0285224914550781\n",
            "Epoch: 0, Step: 900, Loss: 0.9652339220046997\n",
            "Epoch: 0, Step: 950, Loss: 0.8514559268951416\n",
            "Epoch: 0, Step: 1000, Loss: 0.7380262017250061\n",
            "Epoch: 0, Step: 1050, Loss: 0.8800072073936462\n",
            "Epoch: 0, Step: 1100, Loss: 0.6196756958961487\n",
            "Epoch: 0, Step: 1150, Loss: 0.7656455039978027\n",
            "Epoch: 0, Step: 1200, Loss: 0.7850511074066162\n",
            "Epoch: 0, Step: 1250, Loss: 0.577076256275177\n",
            "Epoch: 0, Step: 1300, Loss: 0.9892789125442505\n",
            "Epoch: 0, Step: 1350, Loss: 1.0629758834838867\n",
            "Epoch: 0, Step: 1400, Loss: 0.8447976112365723\n",
            "Epoch: 0, Step: 1450, Loss: 0.8981622457504272\n",
            "Epoch: 0, Step: 1500, Loss: 0.543739914894104\n",
            "Epoch: 0, Step: 1550, Loss: 0.610329270362854\n",
            "Epoch: 0, Step: 1600, Loss: 0.5129790306091309\n",
            "Epoch: 0, Step: 1650, Loss: 0.7620853185653687\n",
            "Epoch: 0, Step: 1700, Loss: 0.6233214735984802\n",
            "Epoch: 0, Step: 1750, Loss: 0.4787514805793762\n",
            "Epoch: 0, Step: 1800, Loss: 0.40541839599609375\n",
            "Epoch: 0, Step: 1850, Loss: 0.6229416728019714\n",
            "Epoch: 0, Step: 1900, Loss: 0.7715606689453125\n",
            "Epoch: 0, Step: 1950, Loss: 0.481478750705719\n",
            "Epoch: 0, Step: 2000, Loss: 0.5334680676460266\n",
            "Epoch: 0, Step: 2050, Loss: 0.6569192409515381\n",
            "Epoch: 0, Step: 2100, Loss: 0.5978952646255493\n",
            "Epoch: 0, Step: 2150, Loss: 0.370277464389801\n",
            "Epoch: 0, Step: 2200, Loss: 0.6663399338722229\n",
            "Epoch: 0, Step: 2250, Loss: 0.4593840539455414\n",
            "Epoch: 0, Step: 2300, Loss: 0.41678163409233093\n",
            "Epoch: 0, Step: 2350, Loss: 0.8058435320854187\n",
            "Epoch: 0, Step: 2400, Loss: 0.35134270787239075\n",
            "Epoch: 0, Step: 2450, Loss: 0.578226625919342\n",
            "Epoch: 0, Step: 2500, Loss: 0.513195812702179\n",
            "Epoch: 0, Step: 2550, Loss: 0.40738245844841003\n",
            "Epoch: 0, Step: 2600, Loss: 0.23775678873062134\n",
            "Epoch: 0, Step: 2650, Loss: 0.671859085559845\n",
            "Epoch: 0, Step: 2700, Loss: 0.6099725961685181\n",
            "Epoch: 0, Step: 2750, Loss: 0.3305201828479767\n",
            "Epoch: 0, Step: 2800, Loss: 0.46996453404426575\n",
            "Epoch: 0, Step: 2850, Loss: 0.5587689280509949\n",
            "Epoch: 0, Step: 2900, Loss: 0.4164791703224182\n",
            "Epoch: 0, Step: 2950, Loss: 0.3485789895057678\n",
            "Epoch: 0, Step: 3000, Loss: 0.6698101162910461\n",
            "Epoch: 0, Step: 3050, Loss: 0.419113427400589\n",
            "Epoch: 0, Step: 3100, Loss: 0.34908321499824524\n",
            "Epoch: 0, Step: 3150, Loss: 0.45800289511680603\n",
            "Epoch: 0, Step: 3200, Loss: 0.34269917011260986\n",
            "Epoch: 0, Step: 3250, Loss: 0.47057268023490906\n",
            "Epoch: 0, Step: 3300, Loss: 0.39288267493247986\n",
            "Epoch: 0, Step: 3350, Loss: 0.41061732172966003\n",
            "Epoch: 0, Step: 3400, Loss: 0.7318477034568787\n",
            "Epoch: 0, Step: 3450, Loss: 0.45586520433425903\n",
            "Epoch: 0, Step: 3500, Loss: 0.4773017466068268\n",
            "Epoch: 0, Step: 3550, Loss: 0.3483106195926666\n",
            "Epoch: 0, Step: 3600, Loss: 0.3651920258998871\n",
            "Epoch: 0, Step: 3650, Loss: 0.3164905309677124\n",
            "Epoch: 0, Step: 3700, Loss: 0.33974286913871765\n",
            "Epoch: 0, Step: 3750, Loss: 0.585034191608429\n",
            "Epoch: 0, Step: 3800, Loss: 0.3252628743648529\n",
            "Epoch: 0, Step: 3850, Loss: 0.6004260182380676\n",
            "Epoch: 0, Step: 3900, Loss: 0.5997352004051208\n",
            "Epoch: 0, Step: 3950, Loss: 0.5297568440437317\n",
            "Epoch: 0, Step: 4000, Loss: 0.4405640661716461\n",
            "Epoch: 0, Step: 4050, Loss: 0.5449736714363098\n",
            "Epoch: 0, Step: 4100, Loss: 0.6059818267822266\n",
            "Epoch: 0, Step: 4150, Loss: 0.5305362939834595\n",
            "Epoch: 0, Step: 4200, Loss: 0.26659518480300903\n",
            "Epoch: 0, Step: 4250, Loss: 0.27074185013771057\n",
            "Epoch: 0, Step: 4300, Loss: 0.3911343216896057\n",
            "Epoch: 0, Step: 4350, Loss: 0.42613646388053894\n",
            "Epoch: 0, Step: 4400, Loss: 0.7231788039207458\n",
            "Epoch: 0, Step: 4450, Loss: 0.45812901854515076\n",
            "Epoch: 0, Step: 4500, Loss: 0.5218308568000793\n",
            "Epoch: 0, Step: 4550, Loss: 0.39277103543281555\n",
            "Epoch: 0, Step: 4600, Loss: 0.3785926103591919\n",
            "Epoch: 0, Step: 4650, Loss: 0.5353166460990906\n",
            "Epoch: 0, Step: 4700, Loss: 0.3926512598991394\n",
            "Epoch: 0, Step: 4750, Loss: 0.5210242867469788\n",
            "Epoch: 0, Step: 4800, Loss: 0.39732253551483154\n",
            "Epoch: 0, Step: 4850, Loss: 0.3943495750427246\n",
            "Epoch: 0, Step: 4900, Loss: 0.48124590516090393\n",
            "Epoch: 0, Step: 4950, Loss: 0.29978832602500916\n",
            "Epoch: 0, Step: 5000, Loss: 0.4840395748615265\n",
            "Epoch: 0, Step: 5050, Loss: 0.3556729257106781\n",
            "Epoch: 0, Step: 5100, Loss: 0.5429540872573853\n",
            "Epoch: 0, Step: 5150, Loss: 0.27358174324035645\n",
            "Epoch: 0, Step: 5200, Loss: 0.3418446183204651\n",
            "Epoch: 0, Step: 5250, Loss: 0.40279248356819153\n",
            "Epoch: 0, Step: 5300, Loss: 0.28650346398353577\n",
            "Epoch: 0, Step: 5350, Loss: 0.3687700629234314\n",
            "Epoch: 0, Step: 5400, Loss: 0.3603798747062683\n",
            "Epoch: 0, Step: 5450, Loss: 0.39431124925613403\n",
            "Epoch: 0, Step: 5500, Loss: 0.40562504529953003\n",
            "Epoch: 0, Step: 5550, Loss: 0.3860090374946594\n",
            "Epoch: 0, Step: 5600, Loss: 0.4848687946796417\n",
            "Epoch: 0, Step: 5650, Loss: 0.23164410889148712\n",
            "Epoch: 0, Step: 5700, Loss: 0.4443379044532776\n",
            "Epoch: 0, Step: 5750, Loss: 0.3743174970149994\n",
            "Epoch: 0, Step: 5800, Loss: 0.3870534300804138\n",
            "Epoch: 0, Step: 5850, Loss: 0.41269755363464355\n",
            "Epoch: 0, Step: 5900, Loss: 0.37042006850242615\n",
            "Epoch: 0, Step: 5950, Loss: 0.7510190606117249\n",
            "Epoch: 1, Step: 0, Loss: 0.4054604768753052\n",
            "Epoch: 1, Step: 50, Loss: 0.20027825236320496\n",
            "Epoch: 1, Step: 100, Loss: 0.5044284462928772\n",
            "Epoch: 1, Step: 150, Loss: 0.3729201853275299\n",
            "Epoch: 1, Step: 200, Loss: 0.40963199734687805\n",
            "Epoch: 1, Step: 250, Loss: 0.6070361733436584\n",
            "Epoch: 1, Step: 300, Loss: 0.24276421964168549\n",
            "Epoch: 1, Step: 350, Loss: 0.39245378971099854\n",
            "Epoch: 1, Step: 400, Loss: 0.2989993393421173\n",
            "Epoch: 1, Step: 450, Loss: 0.24016761779785156\n",
            "Epoch: 1, Step: 500, Loss: 0.2907244563102722\n",
            "Epoch: 1, Step: 550, Loss: 0.3748819828033447\n",
            "Epoch: 1, Step: 600, Loss: 0.37768542766571045\n",
            "Epoch: 1, Step: 650, Loss: 0.3265211582183838\n",
            "Epoch: 1, Step: 700, Loss: 0.45482635498046875\n",
            "Epoch: 1, Step: 750, Loss: 0.29968947172164917\n",
            "Epoch: 1, Step: 800, Loss: 0.4125292897224426\n",
            "Epoch: 1, Step: 850, Loss: 0.16281965374946594\n",
            "Epoch: 1, Step: 900, Loss: 0.3080355226993561\n",
            "Epoch: 1, Step: 950, Loss: 0.2772228419780731\n",
            "Epoch: 1, Step: 1000, Loss: 0.34372812509536743\n",
            "Epoch: 1, Step: 1050, Loss: 0.5185586214065552\n",
            "Epoch: 1, Step: 1100, Loss: 0.4126056432723999\n",
            "Epoch: 1, Step: 1150, Loss: 0.26177188754081726\n",
            "Epoch: 1, Step: 1200, Loss: 0.41433557868003845\n",
            "Epoch: 1, Step: 1250, Loss: 0.3184865117073059\n",
            "Epoch: 1, Step: 1300, Loss: 0.30601873993873596\n",
            "Epoch: 1, Step: 1350, Loss: 0.3230692148208618\n",
            "Epoch: 1, Step: 1400, Loss: 0.4065512716770172\n",
            "Epoch: 1, Step: 1450, Loss: 0.3533313572406769\n",
            "Epoch: 1, Step: 1500, Loss: 0.37231242656707764\n",
            "Epoch: 1, Step: 1550, Loss: 0.3843900263309479\n",
            "Epoch: 1, Step: 1600, Loss: 0.22895365953445435\n",
            "Epoch: 1, Step: 1650, Loss: 0.3158719837665558\n",
            "Epoch: 1, Step: 1700, Loss: 0.2717285454273224\n",
            "Epoch: 1, Step: 1750, Loss: 0.26056134700775146\n",
            "Epoch: 1, Step: 1800, Loss: 0.22297295928001404\n",
            "Epoch: 1, Step: 1850, Loss: 0.22473250329494476\n",
            "Epoch: 1, Step: 1900, Loss: 0.30318722128868103\n",
            "Epoch: 1, Step: 1950, Loss: 0.6516788601875305\n",
            "Epoch: 1, Step: 2000, Loss: 0.2504749298095703\n",
            "Epoch: 1, Step: 2050, Loss: 0.4060874879360199\n",
            "Epoch: 1, Step: 2100, Loss: 0.240613654255867\n",
            "Epoch: 1, Step: 2150, Loss: 0.29151302576065063\n",
            "Epoch: 1, Step: 2200, Loss: 0.25774121284484863\n",
            "Epoch: 1, Step: 2250, Loss: 0.20436431467533112\n",
            "Epoch: 1, Step: 2300, Loss: 0.46755850315093994\n",
            "Epoch: 1, Step: 2350, Loss: 0.40829411149024963\n",
            "Epoch: 1, Step: 2400, Loss: 0.15620586276054382\n",
            "Epoch: 1, Step: 2450, Loss: 0.23360848426818848\n",
            "Epoch: 1, Step: 2500, Loss: 0.2742953896522522\n",
            "Epoch: 1, Step: 2550, Loss: 0.43912985920906067\n",
            "Epoch: 1, Step: 2600, Loss: 0.35743585228919983\n",
            "Epoch: 1, Step: 2650, Loss: 0.4132123589515686\n",
            "Epoch: 1, Step: 2700, Loss: 0.20350006222724915\n",
            "Epoch: 1, Step: 2750, Loss: 0.3748343884944916\n",
            "Epoch: 1, Step: 2800, Loss: 0.4455181658267975\n",
            "Epoch: 1, Step: 2850, Loss: 0.21654817461967468\n",
            "Epoch: 1, Step: 2900, Loss: 0.27090683579444885\n",
            "Epoch: 1, Step: 2950, Loss: 0.4302484691143036\n",
            "Epoch: 1, Step: 3000, Loss: 0.20867057144641876\n",
            "Epoch: 1, Step: 3050, Loss: 0.19814421236515045\n",
            "Epoch: 1, Step: 3100, Loss: 0.1952236443758011\n",
            "Epoch: 1, Step: 3150, Loss: 0.29727235436439514\n",
            "Epoch: 1, Step: 3200, Loss: 0.21448838710784912\n",
            "Epoch: 1, Step: 3250, Loss: 0.4616699814796448\n",
            "Epoch: 1, Step: 3300, Loss: 0.29427170753479004\n",
            "Epoch: 1, Step: 3350, Loss: 0.17678914964199066\n",
            "Epoch: 1, Step: 3400, Loss: 0.499015748500824\n",
            "Epoch: 1, Step: 3450, Loss: 0.2144574373960495\n",
            "Epoch: 1, Step: 3500, Loss: 0.21832875907421112\n",
            "Epoch: 1, Step: 3550, Loss: 0.2519059181213379\n",
            "Epoch: 1, Step: 3600, Loss: 0.39162692427635193\n",
            "Epoch: 1, Step: 3650, Loss: 0.37733033299446106\n",
            "Epoch: 1, Step: 3700, Loss: 0.3914271891117096\n",
            "Epoch: 1, Step: 3750, Loss: 0.39410364627838135\n",
            "Epoch: 1, Step: 3800, Loss: 0.2684732973575592\n",
            "Epoch: 1, Step: 3850, Loss: 0.3614619970321655\n",
            "Epoch: 1, Step: 3900, Loss: 0.31642282009124756\n",
            "Epoch: 1, Step: 3950, Loss: 0.20004132390022278\n",
            "Epoch: 1, Step: 4000, Loss: 0.3453785479068756\n",
            "Epoch: 1, Step: 4050, Loss: 0.19946250319480896\n",
            "Epoch: 1, Step: 4100, Loss: 0.20396320521831512\n",
            "Epoch: 1, Step: 4150, Loss: 0.17432138323783875\n",
            "Epoch: 1, Step: 4200, Loss: 0.22786575555801392\n",
            "Epoch: 1, Step: 4250, Loss: 0.3794570565223694\n",
            "Epoch: 1, Step: 4300, Loss: 0.1453627645969391\n",
            "Epoch: 1, Step: 4350, Loss: 0.34970343112945557\n",
            "Epoch: 1, Step: 4400, Loss: 0.23943991959095\n",
            "Epoch: 1, Step: 4450, Loss: 0.37713637948036194\n",
            "Epoch: 1, Step: 4500, Loss: 0.18484188616275787\n",
            "Epoch: 1, Step: 4550, Loss: 0.4528445601463318\n",
            "Epoch: 1, Step: 4600, Loss: 0.3605204224586487\n",
            "Epoch: 1, Step: 4650, Loss: 0.4525018632411957\n",
            "Epoch: 1, Step: 4700, Loss: 0.20717105269432068\n",
            "Epoch: 1, Step: 4750, Loss: 0.22414296865463257\n",
            "Epoch: 1, Step: 4800, Loss: 0.41771966218948364\n",
            "Epoch: 1, Step: 4850, Loss: 0.3205539584159851\n",
            "Epoch: 1, Step: 4900, Loss: 0.32658544182777405\n",
            "Epoch: 1, Step: 4950, Loss: 0.2998500168323517\n",
            "Epoch: 1, Step: 5000, Loss: 0.201361745595932\n",
            "Epoch: 1, Step: 5050, Loss: 0.45599421858787537\n",
            "Epoch: 1, Step: 5100, Loss: 0.3671473562717438\n",
            "Epoch: 1, Step: 5150, Loss: 0.5164610743522644\n",
            "Epoch: 1, Step: 5200, Loss: 0.29503417015075684\n",
            "Epoch: 1, Step: 5250, Loss: 0.2988687753677368\n",
            "Epoch: 1, Step: 5300, Loss: 0.35760337114334106\n",
            "Epoch: 1, Step: 5350, Loss: 0.46179986000061035\n",
            "Epoch: 1, Step: 5400, Loss: 0.1719677597284317\n",
            "Epoch: 1, Step: 5450, Loss: 0.38938528299331665\n",
            "Epoch: 1, Step: 5500, Loss: 0.21114704012870789\n",
            "Epoch: 1, Step: 5550, Loss: 0.3451712429523468\n",
            "Epoch: 1, Step: 5600, Loss: 0.3565482199192047\n",
            "Epoch: 1, Step: 5650, Loss: 0.29349011182785034\n",
            "Epoch: 1, Step: 5700, Loss: 0.22918318212032318\n",
            "Epoch: 1, Step: 5750, Loss: 0.3312269151210785\n",
            "Epoch: 1, Step: 5800, Loss: 0.3573761284351349\n",
            "Epoch: 1, Step: 5850, Loss: 0.26781830191612244\n",
            "Epoch: 1, Step: 5900, Loss: 0.32792434096336365\n",
            "Epoch: 1, Step: 5950, Loss: 0.5252760052680969\n",
            "Epoch: 2, Step: 0, Loss: 0.5704589486122131\n",
            "Epoch: 2, Step: 50, Loss: 0.19792701303958893\n",
            "Epoch: 2, Step: 100, Loss: 0.4013804495334625\n",
            "Epoch: 2, Step: 150, Loss: 0.1872740238904953\n",
            "Epoch: 2, Step: 200, Loss: 0.2598779797554016\n",
            "Epoch: 2, Step: 250, Loss: 0.24432824552059174\n",
            "Epoch: 2, Step: 300, Loss: 0.24546296894550323\n",
            "Epoch: 2, Step: 350, Loss: 0.3341076672077179\n",
            "Epoch: 2, Step: 400, Loss: 0.21283037960529327\n",
            "Epoch: 2, Step: 450, Loss: 0.3822994828224182\n",
            "Epoch: 2, Step: 500, Loss: 0.11522038280963898\n",
            "Epoch: 2, Step: 550, Loss: 0.16418901085853577\n",
            "Epoch: 2, Step: 600, Loss: 0.2235599011182785\n",
            "Epoch: 2, Step: 650, Loss: 0.23111100494861603\n",
            "Epoch: 2, Step: 700, Loss: 0.30088308453559875\n",
            "Epoch: 2, Step: 750, Loss: 0.18528541922569275\n",
            "Epoch: 2, Step: 800, Loss: 0.3639695644378662\n",
            "Epoch: 2, Step: 850, Loss: 0.24423179030418396\n",
            "Epoch: 2, Step: 900, Loss: 0.1327313482761383\n",
            "Epoch: 2, Step: 950, Loss: 0.5351921916007996\n",
            "Epoch: 2, Step: 1000, Loss: 0.19441857933998108\n",
            "Epoch: 2, Step: 1050, Loss: 0.3420919179916382\n",
            "Epoch: 2, Step: 1100, Loss: 0.3361348509788513\n",
            "Epoch: 2, Step: 1150, Loss: 0.17452041804790497\n",
            "Epoch: 2, Step: 1200, Loss: 0.15780599415302277\n",
            "Epoch: 2, Step: 1250, Loss: 0.26308920979499817\n",
            "Epoch: 2, Step: 1300, Loss: 0.2784830629825592\n",
            "Epoch: 2, Step: 1350, Loss: 0.40674421191215515\n",
            "Epoch: 2, Step: 1400, Loss: 0.31377458572387695\n",
            "Epoch: 2, Step: 1450, Loss: 0.1882748156785965\n",
            "Epoch: 2, Step: 1500, Loss: 0.24576614797115326\n",
            "Epoch: 2, Step: 1550, Loss: 0.21489249169826508\n",
            "Epoch: 2, Step: 1600, Loss: 0.26941385865211487\n",
            "Epoch: 2, Step: 1650, Loss: 0.3178228735923767\n",
            "Epoch: 2, Step: 1700, Loss: 0.14751854538917542\n",
            "Epoch: 2, Step: 1750, Loss: 0.3605938255786896\n",
            "Epoch: 2, Step: 1800, Loss: 0.21617816388607025\n",
            "Epoch: 2, Step: 1850, Loss: 0.3580480217933655\n",
            "Epoch: 2, Step: 1900, Loss: 0.3448244631290436\n",
            "Epoch: 2, Step: 1950, Loss: 0.28154173493385315\n",
            "Epoch: 2, Step: 2000, Loss: 0.29187265038490295\n",
            "Epoch: 2, Step: 2050, Loss: 0.3905769884586334\n",
            "Epoch: 2, Step: 2100, Loss: 0.1870851367712021\n",
            "Epoch: 2, Step: 2150, Loss: 0.27199557423591614\n",
            "Epoch: 2, Step: 2200, Loss: 0.14554442465305328\n",
            "Epoch: 2, Step: 2250, Loss: 0.40006253123283386\n",
            "Epoch: 2, Step: 2300, Loss: 0.2784654200077057\n",
            "Epoch: 2, Step: 2350, Loss: 0.13646993041038513\n",
            "Epoch: 2, Step: 2400, Loss: 0.22411660850048065\n",
            "Epoch: 2, Step: 2450, Loss: 0.3472926616668701\n",
            "Epoch: 2, Step: 2500, Loss: 0.25543251633644104\n",
            "Epoch: 2, Step: 2550, Loss: 0.5223805904388428\n",
            "Epoch: 2, Step: 2600, Loss: 0.22755494713783264\n",
            "Epoch: 2, Step: 2650, Loss: 0.18645009398460388\n",
            "Epoch: 2, Step: 2700, Loss: 0.24041353166103363\n",
            "Epoch: 2, Step: 2750, Loss: 0.1515050232410431\n",
            "Epoch: 2, Step: 2800, Loss: 0.2848362326622009\n",
            "Epoch: 2, Step: 2850, Loss: 0.3315827250480652\n",
            "Epoch: 2, Step: 2900, Loss: 0.2608948349952698\n",
            "Epoch: 2, Step: 2950, Loss: 0.2753618061542511\n",
            "Epoch: 2, Step: 3000, Loss: 0.19178922474384308\n",
            "Epoch: 2, Step: 3050, Loss: 0.19496838748455048\n",
            "Epoch: 2, Step: 3100, Loss: 0.35565412044525146\n",
            "Epoch: 2, Step: 3150, Loss: 0.2722417116165161\n",
            "Epoch: 2, Step: 3200, Loss: 0.1864723563194275\n",
            "Epoch: 2, Step: 3250, Loss: 0.3005150258541107\n",
            "Epoch: 2, Step: 3300, Loss: 0.23394626379013062\n",
            "Epoch: 2, Step: 3350, Loss: 0.36272937059402466\n",
            "Epoch: 2, Step: 3400, Loss: 0.20027968287467957\n",
            "Epoch: 2, Step: 3450, Loss: 0.21672363579273224\n",
            "Epoch: 2, Step: 3500, Loss: 0.35668808221817017\n",
            "Epoch: 2, Step: 3550, Loss: 0.1858198642730713\n",
            "Epoch: 2, Step: 3600, Loss: 0.26334431767463684\n",
            "Epoch: 2, Step: 3650, Loss: 0.310218870639801\n",
            "Epoch: 2, Step: 3700, Loss: 0.38096871972084045\n",
            "Epoch: 2, Step: 3750, Loss: 0.2745758593082428\n",
            "Epoch: 2, Step: 3800, Loss: 0.38063526153564453\n",
            "Epoch: 2, Step: 3850, Loss: 0.2484564483165741\n",
            "Epoch: 2, Step: 3900, Loss: 0.32297584414482117\n",
            "Epoch: 2, Step: 3950, Loss: 0.2039695680141449\n",
            "Epoch: 2, Step: 4000, Loss: 0.22958777844905853\n",
            "Epoch: 2, Step: 4050, Loss: 0.5760372281074524\n",
            "Epoch: 2, Step: 4100, Loss: 0.3349560797214508\n",
            "Epoch: 2, Step: 4150, Loss: 0.24732986092567444\n",
            "Epoch: 2, Step: 4200, Loss: 0.38294970989227295\n",
            "Epoch: 2, Step: 4250, Loss: 0.6270841360092163\n",
            "Epoch: 2, Step: 4300, Loss: 0.34044361114501953\n",
            "Epoch: 2, Step: 4350, Loss: 0.26356539130210876\n",
            "Epoch: 2, Step: 4400, Loss: 0.2935725450515747\n",
            "Epoch: 2, Step: 4450, Loss: 0.25751423835754395\n",
            "Epoch: 2, Step: 4500, Loss: 0.28968122601509094\n",
            "Epoch: 2, Step: 4550, Loss: 0.2790546119213104\n",
            "Epoch: 2, Step: 4600, Loss: 0.18314699828624725\n",
            "Epoch: 2, Step: 4650, Loss: 0.2162085324525833\n",
            "Epoch: 2, Step: 4700, Loss: 0.2934209108352661\n",
            "Epoch: 2, Step: 4750, Loss: 0.26653245091438293\n",
            "Epoch: 2, Step: 4800, Loss: 0.10433261096477509\n",
            "Epoch: 2, Step: 4850, Loss: 0.22065192461013794\n",
            "Epoch: 2, Step: 4900, Loss: 0.33794960379600525\n",
            "Epoch: 2, Step: 4950, Loss: 0.3078681230545044\n",
            "Epoch: 2, Step: 5000, Loss: 0.2649630606174469\n",
            "Epoch: 2, Step: 5050, Loss: 0.2028481513261795\n",
            "Epoch: 2, Step: 5100, Loss: 0.3207331597805023\n",
            "Epoch: 2, Step: 5150, Loss: 0.23309117555618286\n",
            "Epoch: 2, Step: 5200, Loss: 0.3218390941619873\n",
            "Epoch: 2, Step: 5250, Loss: 0.3265072703361511\n",
            "Epoch: 2, Step: 5300, Loss: 0.32219094038009644\n",
            "Epoch: 2, Step: 5350, Loss: 0.26078975200653076\n",
            "Epoch: 2, Step: 5400, Loss: 0.25176990032196045\n",
            "Epoch: 2, Step: 5450, Loss: 0.22232529520988464\n",
            "Epoch: 2, Step: 5500, Loss: 0.28412365913391113\n",
            "Epoch: 2, Step: 5550, Loss: 0.4795018136501312\n",
            "Epoch: 2, Step: 5600, Loss: 0.19594763219356537\n",
            "Epoch: 2, Step: 5650, Loss: 0.20803232491016388\n",
            "Epoch: 2, Step: 5700, Loss: 0.17670972645282745\n",
            "Epoch: 2, Step: 5750, Loss: 0.2392696887254715\n",
            "Epoch: 2, Step: 5800, Loss: 0.25854697823524475\n",
            "Epoch: 2, Step: 5850, Loss: 0.225246861577034\n",
            "Epoch: 2, Step: 5900, Loss: 0.247022345662117\n",
            "Epoch: 2, Step: 5950, Loss: 0.2426680475473404\n",
            "Saving Model...\n",
            "\n",
            "Starting Validation...\n",
            "\n",
            "Completed 0 Steps\n",
            "Completed 150 Steps\n",
            "Completed 300 Steps\n",
            "Completed 450 Steps\n",
            "Completed 600 Steps\n",
            "Completed 750 Steps\n",
            "Completed 900 Steps\n",
            "Completed 1050 Steps\n",
            "Completed 1200 Steps\n",
            "Completed 1350 Steps\n",
            "Validation Completed...\n",
            "\n",
            "Model saved at drive/MyDrive/CS685/t5_trained_model_1639386746.3024917/model_files\n",
            "\n",
            "Generation on Validation data saved at drive/MyDrive/CS685/t5_trained_model_1639386746.3024917/predictions.csv\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Denoising Test Data"
      ],
      "metadata": {
        "id": "FKDJ7pGqD8Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(os.path.join(path, \"xsum_test.csv\"),sep='\\t')\n",
        "test_df['text'] = 'denoise_text: '+test_df['text']\n",
        "\n",
        "saved_model = os.path.join(path, \"t5_trained_model_v1\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(saved_model, local_files_only=True)\n",
        "model = T5ForConditionalGeneration.from_pretrained(saved_model, local_files_only=True).to(device)\n",
        "\n",
        "\n",
        "\n",
        "test_set = GetDataset(\n",
        "        test_df,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text=\"text\",\n",
        "        target_text=\"gt_text\",\n",
        "    )\n",
        "\n",
        "test_loader = DataLoader(test_set, **test_params)\n",
        "predictions, actuals = predict_from_model(model_params[\"TEST_EPOCHS\"], tokenizer, model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w54_9zcRkHQN",
        "outputId": "9bf5f27c-6b31-46a3-ee11-5fd4c876b114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 0 Steps\n",
            "Completed 150 Steps\n",
            "Completed 300 Steps\n",
            "Completed 450 Steps\n",
            "Completed 600 Steps\n",
            "Completed 750 Steps\n",
            "Completed 900 Steps\n",
            "Completed 1050 Steps\n",
            "Completed 1200 Steps\n",
            "Completed 1350 Steps\n",
            "Completed 1500 Steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds  = pd.DataFrame({\"Predicted_Text\": predictions, \"GT_Text\": actuals})\n",
        "test_preds.to_csv(os.path.join(path, \"xsum_test_pred_t5.csv\"), index=False, sep='\\t')"
      ],
      "metadata": {
        "id": "kRY0t1ZRzyA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation on Test Data"
      ],
      "metadata": {
        "id": "yx3RvDVVYMX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(os.path.join(path, \"xsum_test.csv\"),sep='\\t')\n",
        "test_preds = pd.read_csv(os.path.join(path, \"xsum_test_pred_t5.csv\"), sep='\\t')\n",
        "acc_df = get_accuracy_df(test_df, test_preds)\n",
        "\n",
        "print(f\"Total incorrect tokens: {acc_df['replaced_word_count'].sum()}\\n Total correct predictions: {acc_df['correct_prediction_count'].sum()} \\nTest accuracy: {acc_df['correct_prediction_count'].sum()/acc_df['replaced_word_count'].sum()}\")\n",
        "display(acc_df.head(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "n-AEsn9eYSSA",
        "outputId": "6a2965b9-3da8-4597-b415-e20ac471761b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total incorrect tokens: 63595\n",
            " Total correct predictions: 55858 \n",
            "Test accuracy: 0.8783394920984354\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>Predicted_Text</th>\n",
              "      <th>GT_Text</th>\n",
              "      <th>replaced_gt_words</th>\n",
              "      <th>replaced_original_words</th>\n",
              "      <th>replaced_word_count</th>\n",
              "      <th>correct_predictions</th>\n",
              "      <th>correct_prediction_count</th>\n",
              "      <th>wrong_predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bangor City MAnageR Kevin Nicholson sayS it Would be a hg achievement if they overturn a 1-0 first-leg dficit against Lyngby BK.</td>\n",
              "      <td>Bangor City manager Kevin Nicholson says it would be a huge achievement if they overturn a 1-0 first-leg deficit against Lyngby BK.</td>\n",
              "      <td>Bangor City manager Kevin Nicholson says it would be a huge achievement if they overturn a 1-0 first-leg deficit against Lyngby BK.</td>\n",
              "      <td>[huge, deficit]</td>\n",
              "      <td>[hg, dficit]</td>\n",
              "      <td>2</td>\n",
              "      <td>[huge, deficit]</td>\n",
              "      <td>2</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dee actor who played Darth Vader in the original Star Wars filmss says hopeufls from Bristol auditioning for the nww film should disgis their accents.</td>\n",
              "      <td>The actor who played Darth Vader in the original Star Wars films says hopefuls from Bristol auditioning for the new film should disgise their accents.</td>\n",
              "      <td>The actor who played Darth Vader in the original Star Wars films says hopefuls from Bristol auditioning for the new film should disguise their accents.</td>\n",
              "      <td>[the, films, hopefuls, new, disguise]</td>\n",
              "      <td>[dee, filmss, hopeufls, nww, disgis]</td>\n",
              "      <td>5</td>\n",
              "      <td>[the, films, hopefuls, new]</td>\n",
              "      <td>4</td>\n",
              "      <td>[disgise]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                     text  \\\n",
              "0  Bangor City MAnageR Kevin Nicholson sayS it Would be a hg achievement if they overturn a 1-0 first-leg dficit against Lyngby BK.                         \n",
              "1  dee actor who played Darth Vader in the original Star Wars filmss says hopeufls from Bristol auditioning for the nww film should disgis their accents.   \n",
              "\n",
              "                                                                                                                                           Predicted_Text  \\\n",
              "0  Bangor City manager Kevin Nicholson says it would be a huge achievement if they overturn a 1-0 first-leg deficit against Lyngby BK.                      \n",
              "1  The actor who played Darth Vader in the original Star Wars films says hopefuls from Bristol auditioning for the new film should disgise their accents.   \n",
              "\n",
              "                                                                                                                                                   GT_Text  \\\n",
              "0  Bangor City manager Kevin Nicholson says it would be a huge achievement if they overturn a 1-0 first-leg deficit against Lyngby BK.                       \n",
              "1  The actor who played Darth Vader in the original Star Wars films says hopefuls from Bristol auditioning for the new film should disguise their accents.   \n",
              "\n",
              "                       replaced_gt_words  \\\n",
              "0  [huge, deficit]                         \n",
              "1  [the, films, hopefuls, new, disguise]   \n",
              "\n",
              "                replaced_original_words  replaced_word_count  \\\n",
              "0  [hg, dficit]                          2                     \n",
              "1  [dee, filmss, hopeufls, nww, disgis]  5                     \n",
              "\n",
              "           correct_predictions  correct_prediction_count wrong_predictions  \n",
              "0  [huge, deficit]              2                         []                \n",
              "1  [the, films, hopefuls, new]  4                         [disgise]         "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correctly denoised sentences"
      ],
      "metadata": {
        "id": "h0TAIzoG9Sad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_df[acc_df['replaced_word_count']==acc_df['correct_prediction_count']]\\\n",
        "  .drop(columns=['replaced_original_words','replaced_word_count',\n",
        "               'correct_prediction_count','wrong_predictions']).head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "m_y8inyY7Zyn",
        "outputId": "8a1cde55-b39e-4273-eb14-17f8a294a886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>Predicted_Text</th>\n",
              "      <th>GT_Text</th>\n",
              "      <th>replaced_gt_words</th>\n",
              "      <th>correct_predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bangor City MAnageR Kevin Nicholson sayS it Would be a hg achievement if they overturn a 1-0 first-leg dficit against Lyngby BK.</td>\n",
              "      <td>Bangor City manager Kevin Nicholson says it would be a huge achievement if they overturn a 1-0 first-leg deficit against Lyngby BK.</td>\n",
              "      <td>Bangor City manager Kevin Nicholson says it would be a huge achievement if they overturn a 1-0 first-leg deficit against Lyngby BK.</td>\n",
              "      <td>[huge, deficit]</td>\n",
              "      <td>[huge, deficit]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Catalans Dragons earned a narrow vic2ry at Hull FC, despite four tRies from Airlie Birds wingEr Tom Lineham.</td>\n",
              "      <td>Catalans Dragons earned a narrow victory at Hull FC, despite four goals from Airlie Birds winger Tom Lineham.</td>\n",
              "      <td>Catalans Dragons earned a narrow victory at Hull FC, despite four tries from Airlie Birds winger Tom Lineham.</td>\n",
              "      <td>[victory]</td>\n",
              "      <td>[victory]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A driverr whoooo crashed into a shop, klilingg a teenage passenger, after failig t stopppppp for police has been jiled for eight years.</td>\n",
              "      <td>A driver who crashed into a shop, killing a teenage passenger, after failing to stop for police has been jailed for eight years.</td>\n",
              "      <td>A driver who crashed into a shop, killing a teenage passenger, after failing to stop for police has been jailed for eight years.</td>\n",
              "      <td>[driver, who, killing, failing, to, stop, jailed]</td>\n",
              "      <td>[driver, who, killing, failing, to, stop, jailed]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Kenyan police have bEen Involved in aaa firece gun battle with militant Islamists in north-eastern Garissa County, leaving onee officer wounded, a government spokesman has told the BBC.</td>\n",
              "      <td>Kenyan police have been involved in a fierce gun battle with militant Islamists in north-eastern Garissa County, leaving one officer wounded, a government spokesman has told the BBC.</td>\n",
              "      <td>Kenyan police have been involved in a fierce gun battle with militant Islamists in north-eastern Garissa County, leaving one officer wounded, a government spokesman has told the BBC.</td>\n",
              "      <td>[a, fierce, one]</td>\n",
              "      <td>[a, fierce, one]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>The care provided for vulnrble chIldrEn in Leeds has been praised by gvrnment inspectors fve years after the SrviceS were rated inadeqUate.</td>\n",
              "      <td>The care provided for vulnerable children in Leeds has been praised by government inspectors five years after the services were rated inadequate.</td>\n",
              "      <td>The care provided for vulnerable children in Leeds has been praised by government inspectors five years after the services were rated inadequate.</td>\n",
              "      <td>[vulnerable, government, five, services]</td>\n",
              "      <td>[vulnerable, government, five, services]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                        text  \\\n",
              "0  Bangor City MAnageR Kevin Nicholson sayS it Would be a hg achievement if they overturn a 1-0 first-leg dficit against Lyngby BK.                                                            \n",
              "2  Catalans Dragons earned a narrow vic2ry at Hull FC, despite four tRies from Airlie Birds wingEr Tom Lineham.                                                                                \n",
              "3  A driverr whoooo crashed into a shop, klilingg a teenage passenger, after failig t stopppppp for police has been jiled for eight years.                                                     \n",
              "4  Kenyan police have bEen Involved in aaa firece gun battle with militant Islamists in north-eastern Garissa County, leaving onee officer wounded, a government spokesman has told the BBC.   \n",
              "9  The care provided for vulnrble chIldrEn in Leeds has been praised by gvrnment inspectors fve years after the SrviceS were rated inadeqUate.                                                 \n",
              "\n",
              "                                                                                                                                                                           Predicted_Text  \\\n",
              "0  Bangor City manager Kevin Nicholson says it would be a huge achievement if they overturn a 1-0 first-leg deficit against Lyngby BK.                                                      \n",
              "2  Catalans Dragons earned a narrow victory at Hull FC, despite four goals from Airlie Birds winger Tom Lineham.                                                                            \n",
              "3  A driver who crashed into a shop, killing a teenage passenger, after failing to stop for police has been jailed for eight years.                                                         \n",
              "4  Kenyan police have been involved in a fierce gun battle with militant Islamists in north-eastern Garissa County, leaving one officer wounded, a government spokesman has told the BBC.   \n",
              "9  The care provided for vulnerable children in Leeds has been praised by government inspectors five years after the services were rated inadequate.                                        \n",
              "\n",
              "                                                                                                                                                                                  GT_Text  \\\n",
              "0  Bangor City manager Kevin Nicholson says it would be a huge achievement if they overturn a 1-0 first-leg deficit against Lyngby BK.                                                      \n",
              "2  Catalans Dragons earned a narrow victory at Hull FC, despite four tries from Airlie Birds winger Tom Lineham.                                                                            \n",
              "3  A driver who crashed into a shop, killing a teenage passenger, after failing to stop for police has been jailed for eight years.                                                         \n",
              "4  Kenyan police have been involved in a fierce gun battle with militant Islamists in north-eastern Garissa County, leaving one officer wounded, a government spokesman has told the BBC.   \n",
              "9  The care provided for vulnerable children in Leeds has been praised by government inspectors five years after the services were rated inadequate.                                        \n",
              "\n",
              "                                   replaced_gt_words  \\\n",
              "0  [huge, deficit]                                     \n",
              "2  [victory]                                           \n",
              "3  [driver, who, killing, failing, to, stop, jailed]   \n",
              "4  [a, fierce, one]                                    \n",
              "9  [vulnerable, government, five, services]            \n",
              "\n",
              "                                 correct_predictions  \n",
              "0  [huge, deficit]                                    \n",
              "2  [victory]                                          \n",
              "3  [driver, who, killing, failing, to, stop, jailed]  \n",
              "4  [a, fierce, one]                                   \n",
              "9  [vulnerable, government, five, services]           "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Incorrectly denoised sentences"
      ],
      "metadata": {
        "id": "v-HKrDTS-p92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect_df = acc_df[acc_df['replaced_word_count']!=acc_df['correct_prediction_count']]\\\n",
        "  .drop(columns=['replaced_original_words','replaced_word_count',\n",
        "               'correct_prediction_count'])\n",
        "incorrect_df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "lXRzItcv-uIm",
        "outputId": "f99b1bcb-33ea-4665-afca-66dd603327a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>Predicted_Text</th>\n",
              "      <th>GT_Text</th>\n",
              "      <th>replaced_gt_words</th>\n",
              "      <th>correct_predictions</th>\n",
              "      <th>wrong_predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dee actor who played Darth Vader in the original Star Wars filmss says hopeufls from Bristol auditioning for the nww film should disgis their accents.</td>\n",
              "      <td>The actor who played Darth Vader in the original Star Wars films says hopefuls from Bristol auditioning for the new film should disgise their accents.</td>\n",
              "      <td>The actor who played Darth Vader in the original Star Wars films says hopefuls from Bristol auditioning for the new film should disguise their accents.</td>\n",
              "      <td>[the, films, hopefuls, new, disguise]</td>\n",
              "      <td>[the, films, hopefuls, new]</td>\n",
              "      <td>[disgise]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Leicester Haveee complained t the Football Association about abuse, provcoatoin and intimidation thy say their players, fans and stafff suffered during Sturdays match at Millwall.</td>\n",
              "      <td>Leicester have complained to the Football Association about abuse, provision and intimidation they say their players, fans and staff suffered during Saturdays match at Millwall.</td>\n",
              "      <td>Leicester have complained to the Football Association about abuse, provocation and intimidation they say their players, fans and staff suffered during Saturdays match at Millwall.</td>\n",
              "      <td>[have, to, provocation, they, staff, saturdays]</td>\n",
              "      <td>[have, to, they, staff, saturdays]</td>\n",
              "      <td>[provision]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Ruth Davidson hasss wrned de SNP aganist using Brexit as an excuse to strtt yet anthr fratricidal conflict.</td>\n",
              "      <td>Ruth Davidson has warned the SNP activist using Brexit as an excuse to strike yet another religious conflict.</td>\n",
              "      <td>Ruth Davidson has warned the SNP against using Brexit as an excuse to start yet another fratricidal conflict.</td>\n",
              "      <td>[has, warned, the, against, start, another]</td>\n",
              "      <td>[has, warned, the, another]</td>\n",
              "      <td>[activist, strike]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The lss fff midfielder NGolo Kante to Chelsea maens Leicester wiilllllllll Not repe8 their shock title win of last season, sys formr Foxes striker Gary Lineker.</td>\n",
              "      <td>The loss of midfielder NGolo Kante to Chelsea means Leicester will not replicate their shock title win of last season, says former Foxes striker Gary Lineker.</td>\n",
              "      <td>The loss of midfielder NGolo Kante to Chelsea means Leicester will not repeat their shock title win of last season, says former Foxes striker Gary Lineker.</td>\n",
              "      <td>[loss, of, means, will, repeat, says, former]</td>\n",
              "      <td>[loss, of, means, will, says, former]</td>\n",
              "      <td>[replicate]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Cch John Bracewell says the looming decision over Test status is puttng his Ireland team under huge pressure innn the one-day series against England.</td>\n",
              "      <td>Coach John Bracewell says the looming decision over Test status is placing his Ireland team under huge pressure in the one-day series against England.</td>\n",
              "      <td>Coach John Bracewell says the looming decision over Test status is putting his Ireland team under huge pressure in the one-day series against England.</td>\n",
              "      <td>[coach, putting, in]</td>\n",
              "      <td>[coach, in]</td>\n",
              "      <td>[placing]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Satuurdays singr Rochelle Humes is the New host of The X Factor spin-off sHOw The Xtra Factor, whre she hass been prtnered with Kiss FM DJ Melvin Odoom.</td>\n",
              "      <td>Saturdays singer Rochelle Humes is the new host of The X Factor spin-off Show The Extra Factor, where she has been replaced with Kiss FM DJ Melvin Odoom.</td>\n",
              "      <td>Saturdays singer Rochelle Humes is the new host of The X Factor spin-off show The Xtra Factor, where she has been partnered with Kiss FM DJ Melvin Odoom.</td>\n",
              "      <td>[saturdays, singer, where, has, partnered]</td>\n",
              "      <td>[saturdays, singer, where, has]</td>\n",
              "      <td>[replaced]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>A seacrh hass resuemd of the River Ness in Inverness after police received reports of a man falling from a bridge oN Sunday.</td>\n",
              "      <td>A seacrooper has resurfaced of the River Ness in Inverness after police received reports of a man falling from a bridge on Sunday.</td>\n",
              "      <td>A search has resumed of the River Ness in Inverness after police received reports of a man falling from a bridge on Sunday.</td>\n",
              "      <td>[search, has, resumed]</td>\n",
              "      <td>[has]</td>\n",
              "      <td>[seacrooper, resurfaced]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Owen Williams kikced 17 points s Leicester Tigers inflicted a fisrt defat of d season on Bath.</td>\n",
              "      <td>Owen Williams scored 17 points as Leicester Tigers inflicted a first defeat of the season on Bath.</td>\n",
              "      <td>Owen Williams kicked 17 points as Leicester Tigers inflicted a first defeat of the season on Bath.</td>\n",
              "      <td>[kicked, as, first, defeat, the]</td>\n",
              "      <td>[as, first, defeat, the]</td>\n",
              "      <td>[scored]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Inexperienced refrees have gone berserk nn imposnnng yellow cards afer a neww rule Was intrdcddd in 2017, sys ex-Wales sr Jonathan Davies.</td>\n",
              "      <td>Inexperienced red cards have gone berserk in imposing yellow cards after a new rule was introduced in 2017, says ex-Wales striker Jonathan Davies.</td>\n",
              "      <td>Inexperienced referees have gone berserk in imposing yellow cards after a new rule was introduced in 2017, says ex-Wales star Jonathan Davies.</td>\n",
              "      <td>[referees, in imposing, after, new, introduced, says, star]</td>\n",
              "      <td>[after, in imposing, introduced, says, new]</td>\n",
              "      <td>[red cards, striker]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>The famous Spiegleetntttt has lost its hme during the Edinburgh Festival.</td>\n",
              "      <td>The famous Spiegleetint has lost its name during the Edinburgh Festival.</td>\n",
              "      <td>The famous Spiegeltent has lost its home during the Edinburgh Festival.</td>\n",
              "      <td>[spiegeltent, home]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[spiegleetint, name]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                   text  \\\n",
              "1   dee actor who played Darth Vader in the original Star Wars filmss says hopeufls from Bristol auditioning for the nww film should disgis their accents.                                \n",
              "5   Leicester Haveee complained t the Football Association about abuse, provcoatoin and intimidation thy say their players, fans and stafff suffered during Sturdays match at Millwall.   \n",
              "6   Ruth Davidson hasss wrned de SNP aganist using Brexit as an excuse to strtt yet anthr fratricidal conflict.                                                                           \n",
              "7   The lss fff midfielder NGolo Kante to Chelsea maens Leicester wiilllllllll Not repe8 their shock title win of last season, sys formr Foxes striker Gary Lineker.                      \n",
              "8   Cch John Bracewell says the looming decision over Test status is puttng his Ireland team under huge pressure innn the one-day series against England.                                 \n",
              "10  Satuurdays singr Rochelle Humes is the New host of The X Factor spin-off sHOw The Xtra Factor, whre she hass been prtnered with Kiss FM DJ Melvin Odoom.                              \n",
              "12  A seacrh hass resuemd of the River Ness in Inverness after police received reports of a man falling from a bridge oN Sunday.                                                          \n",
              "17  Owen Williams kikced 17 points s Leicester Tigers inflicted a fisrt defat of d season on Bath.                                                                                        \n",
              "18  Inexperienced refrees have gone berserk nn imposnnng yellow cards afer a neww rule Was intrdcddd in 2017, sys ex-Wales sr Jonathan Davies.                                            \n",
              "19  The famous Spiegleetntttt has lost its hme during the Edinburgh Festival.                                                                                                             \n",
              "\n",
              "                                                                                                                                                                       Predicted_Text  \\\n",
              "1   The actor who played Darth Vader in the original Star Wars films says hopefuls from Bristol auditioning for the new film should disgise their accents.                              \n",
              "5   Leicester have complained to the Football Association about abuse, provision and intimidation they say their players, fans and staff suffered during Saturdays match at Millwall.   \n",
              "6   Ruth Davidson has warned the SNP activist using Brexit as an excuse to strike yet another religious conflict.                                                                       \n",
              "7   The loss of midfielder NGolo Kante to Chelsea means Leicester will not replicate their shock title win of last season, says former Foxes striker Gary Lineker.                      \n",
              "8   Coach John Bracewell says the looming decision over Test status is placing his Ireland team under huge pressure in the one-day series against England.                              \n",
              "10  Saturdays singer Rochelle Humes is the new host of The X Factor spin-off Show The Extra Factor, where she has been replaced with Kiss FM DJ Melvin Odoom.                           \n",
              "12  A seacrooper has resurfaced of the River Ness in Inverness after police received reports of a man falling from a bridge on Sunday.                                                  \n",
              "17  Owen Williams scored 17 points as Leicester Tigers inflicted a first defeat of the season on Bath.                                                                                  \n",
              "18  Inexperienced red cards have gone berserk in imposing yellow cards after a new rule was introduced in 2017, says ex-Wales striker Jonathan Davies.                                  \n",
              "19  The famous Spiegleetint has lost its name during the Edinburgh Festival.                                                                                                            \n",
              "\n",
              "                                                                                                                                                                                GT_Text  \\\n",
              "1   The actor who played Darth Vader in the original Star Wars films says hopefuls from Bristol auditioning for the new film should disguise their accents.                               \n",
              "5   Leicester have complained to the Football Association about abuse, provocation and intimidation they say their players, fans and staff suffered during Saturdays match at Millwall.   \n",
              "6   Ruth Davidson has warned the SNP against using Brexit as an excuse to start yet another fratricidal conflict.                                                                         \n",
              "7   The loss of midfielder NGolo Kante to Chelsea means Leicester will not repeat their shock title win of last season, says former Foxes striker Gary Lineker.                           \n",
              "8   Coach John Bracewell says the looming decision over Test status is putting his Ireland team under huge pressure in the one-day series against England.                                \n",
              "10  Saturdays singer Rochelle Humes is the new host of The X Factor spin-off show The Xtra Factor, where she has been partnered with Kiss FM DJ Melvin Odoom.                             \n",
              "12  A search has resumed of the River Ness in Inverness after police received reports of a man falling from a bridge on Sunday.                                                           \n",
              "17  Owen Williams kicked 17 points as Leicester Tigers inflicted a first defeat of the season on Bath.                                                                                    \n",
              "18  Inexperienced referees have gone berserk in imposing yellow cards after a new rule was introduced in 2017, says ex-Wales star Jonathan Davies.                                        \n",
              "19  The famous Spiegeltent has lost its home during the Edinburgh Festival.                                                                                                               \n",
              "\n",
              "                                              replaced_gt_words  \\\n",
              "1   [the, films, hopefuls, new, disguise]                         \n",
              "5   [have, to, provocation, they, staff, saturdays]               \n",
              "6   [has, warned, the, against, start, another]                   \n",
              "7   [loss, of, means, will, repeat, says, former]                 \n",
              "8   [coach, putting, in]                                          \n",
              "10  [saturdays, singer, where, has, partnered]                    \n",
              "12  [search, has, resumed]                                        \n",
              "17  [kicked, as, first, defeat, the]                              \n",
              "18  [referees, in imposing, after, new, introduced, says, star]   \n",
              "19  [spiegeltent, home]                                           \n",
              "\n",
              "                            correct_predictions         wrong_predictions  \n",
              "1   [the, films, hopefuls, new]                  [disgise]                 \n",
              "5   [have, to, they, staff, saturdays]           [provision]               \n",
              "6   [has, warned, the, another]                  [activist, strike]        \n",
              "7   [loss, of, means, will, says, former]        [replicate]               \n",
              "8   [coach, in]                                  [placing]                 \n",
              "10  [saturdays, singer, where, has]              [replaced]                \n",
              "12  [has]                                        [seacrooper, resurfaced]  \n",
              "17  [as, first, defeat, the]                     [scored]                  \n",
              "18  [after, in imposing, introduced, says, new]  [red cards, striker]      \n",
              "19  []                                           [spiegleetint, name]      "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# writing incorrect examples for analysis\n",
        "incorrect_df.to_csv(os.path.join(path, \"xsum_test_wrong_pred_t5.csv\"), index=False, sep='\\t')"
      ],
      "metadata": {
        "id": "GufI7tMw_I5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing SMS data"
      ],
      "metadata": {
        "id": "tls9DcDetRIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = os.path.join(path, \"t5_trained_model_v1\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(saved_model, local_files_only=True)\n",
        "model = T5ForConditionalGeneration.from_pretrained(saved_model, local_files_only=True).to(device)\n",
        "\n",
        "sms_df = pd.read_csv(os.path.join(path, \"sms_data.tsv\"),sep='\\t')\n",
        "sms_df['text'] = 'denoise_text: '+sms_df['text']\n",
        "sms_df['gt_text']=\"\"\n",
        "\n",
        "sms_set = GetDataset(\n",
        "        sms_df,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text=\"text\",\n",
        "        target_text=\"gt_text\",\n",
        "    )\n",
        "\n",
        "data_loader = DataLoader(sms_set, **test_params)\n",
        "predictions, actuals = predict_from_model(model_params[\"TEST_EPOCHS\"], tokenizer, model, data_loader)\n",
        "\n",
        "sms_preds  = pd.DataFrame({\"Predicted_Text\": predictions})\n",
        "sms_preds.to_csv(os.path.join(path, \"sms_test_pred_t5.csv\"), index=False, sep='\\t')\n"
      ],
      "metadata": {
        "id": "2baJAiGGtYvT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeab9337-f722-4a25-c257-8a10021d49cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 0 Steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing data for Sentiment Analysis"
      ],
      "metadata": {
        "id": "ZgtL70gytZRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = os.path.join(path, \"t5_trained_model_v1\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(saved_model, local_files_only=True)\n",
        "model = T5ForConditionalGeneration.from_pretrained(saved_model, local_files_only=True).to(device)\n",
        "\n",
        "sst_df = pd.read_csv(os.path.join(path, \"sst_unnormalized_data.tsv\"),sep='\\t')\n",
        "sst_df['sentence'] = 'denoise_text: '+sst_df['sentence']\n",
        "\n",
        "sst_set = GetDataset(\n",
        "        sst_df,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text=\"sentence\",\n",
        "        target_text=\"label\",\n",
        "    )\n",
        "\n",
        "data_loader = DataLoader(sst_set, **test_params)\n",
        "predictions, actuals = predict_from_model(model_params[\"TEST_EPOCHS\"], tokenizer, model, data_loader)\n",
        "\n",
        "sst_preds  = pd.DataFrame({\"sentence\": predictions, \"label\": actuals})\n",
        "sst_preds.to_csv(os.path.join(path, \"t5_sst_normalized.csv\"), index=False, sep='\\t')\n"
      ],
      "metadata": {
        "id": "bDxkPhrHtl2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ccc9578-3aa7-4dc9-9326-8c5eeccb8017"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 0 Steps\n"
          ]
        }
      ]
    }
  ]
}